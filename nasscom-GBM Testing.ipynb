{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "URL = 'Health_care_Dataset_for_probelm.xlsx'\n",
    "df_training = pandas.read_excel(URL,sheetname=1)\n",
    "df_testing = pandas.read_excel(URL, sheetname=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_training[:]\n",
    "df_test = df_testing[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = df_train['Lung_Cancer'].values\n",
    "del df_train['Lung_Cancer']\n",
    "del df_test['Lung_Cancer']\n",
    "del df_train['Patient_ID']\n",
    "del df_test['Patient_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kowshik\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Kowshik\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor1\n",
      "Factor5\n",
      "DiseaseHis1\n",
      "DiseaseHis2\n",
      "DiseaseHis3\n",
      "DiseaseHis4\n",
      "DiseaseHis5\n",
      "DiseaseHis7\n",
      "DiseaseStage1\n",
      "LungFunct19\n",
      "Disease1\n",
      "Disease1Treat\n",
      "Disease2\n",
      "Disease3\n",
      "Disease4\n",
      "Disease4Treat\n",
      "Disease5\n",
      "Disease5Treat\n",
      "Disease6\n",
      "Disease6Treat\n",
      "Disease7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in list(df_train.columns.values):\n",
    "    if len(np.unique(df_train[i].values)) < 5:\n",
    "        df_train[i] = df_train[i].astype(str)\n",
    "import numpy as np\n",
    "for i in list(df_test.columns.values):\n",
    "    if len(np.unique(df_test[i].values)) < 5:\n",
    "        df_test[i] = df_test[i].astype(str)\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ENCODING\n",
    "X_Tr = df_train.to_dict('records')\n",
    "X_Te = df_test.to_dict('records')\n",
    "X_tr = []\n",
    "X_te = []\n",
    "X_tr.extend(X_Tr)\n",
    "X_te.extend(X_Te)\n",
    "X_total = X_tr + X_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding \n",
    "from sklearn.feature_extraction import DictVectorizer \n",
    "enc = DictVectorizer()\n",
    "X_encoded_total =enc.fit_transform(X_total)\n",
    "X_encoded_train =X_encoded_total[:len(X_tr)]\n",
    "X_train = X_encoded_train[:int(0.999*len(X_encoded_train.toarray()))].toarray()\n",
    "X_test = X_encoded_train[int(0.999*len(X_encoded_train.toarray())):].toarray()\n",
    "y_train = Y_train[:int(0.999*len(X_encoded_train.toarray()))]\n",
    "y_test  = Y_train[int(0.999*len(X_encoded_train.toarray())):]\n",
    "X_encoded_test =X_encoded_total[len(X_tr):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_encoded_train,Y_train)\n",
    "Importance = clf.feature_importances_*100\n",
    "Headings = sorted(X_Te[0].keys())\n",
    "FeatureImportance =  dict(zip( Headings,Importance))\n",
    "FeatureImportanceSorted = (sorted(FeatureImportance.items(), key=lambda t: t[1]))\n",
    "Headings_sorted =[]\n",
    "Importance_sorted = []\n",
    "for i in range(len(FeatureImportanceSorted)):\n",
    "    Headings_sorted.append(FeatureImportanceSorted[i][0])\n",
    "    Importance_sorted.append(FeatureImportanceSorted[i][1])\n",
    "Headings_sorted = list(Headings_sorted[::-1])\n",
    "Importance_sorted = list(Importance_sorted[::-1])\n",
    "labels = Headings_sorted[:10]\n",
    "labels.append('other')\n",
    "slices = Importance_sorted[:10]\n",
    "slices.append(sum(Importance_sorted[10:]))\n",
    "#Plotting The Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Ques2',\n",
       " u'Disease2Times',\n",
       " u'LungFunct7',\n",
       " u'Factor6',\n",
       " u'LungFunct18',\n",
       " u'Smoke3',\n",
       " u'LungFunct13',\n",
       " u'LungFunct8',\n",
       " u'Factor3',\n",
       " u'Ques1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Headings_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pandas.DataFrame(Headings_sorted)\n",
    "df1['1'] = Importance_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "writer = ExcelWriter('importances.xlsx')\n",
    "df1.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-2cf3a104bdb9>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-44-2cf3a104bdb9>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    params = {'n_estimators': 500, {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 1}}\u001b[0m\n\u001b[1;37m                                                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "params = {'n_estimators': 500, {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 1}}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "n_estimators = params['n_estimators']\n",
    "x = np.arange(n_estimators) + 1\n",
    "def heldout_score(clf, X_test, y_test):\n",
    "    \"\"\"compute deviance scores on ``X_test`` and ``y_test``. \"\"\"\n",
    "    score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        score[i] = clf.loss_(y_test, y_pred)\n",
    "    return score\n",
    "def cv_estimate(n_folds=3):\n",
    "    cv = KFold(n=X_train.shape[0], n_folds=n_folds)\n",
    "    cv_clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for train, test in cv:\n",
    "        cv_clf.fit(X_train[train], y_train[train])\n",
    "        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n",
    "    val_scores /= n_folds\n",
    "    return val_scores\n",
    "cv_score = cv_estimate(3)\n",
    "test_score = heldout_score(clf, X_test, y_test)\n",
    "cumsum = -np.cumsum(clf.oob_improvement_)\n",
    "oob_best_iter = x[np.argmin(cumsum)]\n",
    "test_score -= test_score[0]\n",
    "test_best_iter = x[np.argmin(test_score)]\n",
    "cv_score -= cv_score[0]\n",
    "cv_best_iter = x[np.argmin(cv_score)]\n",
    "oob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\n",
    "test_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\n",
    "cv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot curves and vertical lines for best iterations\n",
    "plt.plot(x, cumsum, label='OOB loss', color=oob_color)\n",
    "plt.plot(x, test_score, label='Test loss', color=test_color)\n",
    "plt.plot(x, cv_score, label='CV loss', color=cv_color)\n",
    "plt.axvline(x=oob_best_iter, color=oob_color)\n",
    "plt.axvline(x=test_best_iter, color=test_color)\n",
    "plt.axvline(x=cv_best_iter, color=cv_color)\n",
    "\n",
    "# add three vertical lines to xticks\n",
    "xticks = plt.xticks()\n",
    "xticks_pos = np.array(xticks[0].tolist() +\n",
    "                      [oob_best_iter, cv_best_iter, test_best_iter])\n",
    "xticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +\n",
    "                        ['OOB', 'CV', 'Test'])\n",
    "ind = np.argsort(xticks_pos)\n",
    "xticks_pos = xticks_pos[ind]\n",
    "xticks_label = xticks_label[ind]\n",
    "plt.xticks(xticks_pos, xticks_label)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('normalized loss')\n",
    "plt.xlabel('number of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.832 (+/-0.064) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 1}\n",
      "0.839 (+/-0.041) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 2}\n",
      "0.834 (+/-0.046) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 3}\n",
      "0.831 (+/-0.045) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 4}\n",
      "0.849 (+/-0.064) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 1}\n",
      "0.834 (+/-0.058) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 2}\n",
      "0.839 (+/-0.055) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 3}\n",
      "0.839 (+/-0.054) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 4}\n",
      "0.842 (+/-0.048) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 1}\n",
      "0.831 (+/-0.026) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 2}\n",
      "0.836 (+/-0.045) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 3}\n",
      "0.832 (+/-0.044) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 4}\n",
      "0.844 (+/-0.053) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 1}\n",
      "0.830 (+/-0.041) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 2}\n",
      "0.833 (+/-0.049) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 3}\n",
      "0.836 (+/-0.044) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 4}\n",
      "0.840 (+/-0.045) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 1}\n",
      "0.830 (+/-0.021) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 2}\n",
      "0.826 (+/-0.048) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 3}\n",
      "0.828 (+/-0.046) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 4}\n",
      "0.836 (+/-0.050) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 1}\n",
      "0.835 (+/-0.033) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 2}\n",
      "0.837 (+/-0.055) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 3}\n",
      "0.836 (+/-0.049) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 4}\n",
      "0.832 (+/-0.019) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 1}\n",
      "0.830 (+/-0.042) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 2}\n",
      "0.830 (+/-0.042) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 3}\n",
      "0.826 (+/-0.025) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 4}\n",
      "0.842 (+/-0.045) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 1}\n",
      "0.835 (+/-0.041) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 2}\n",
      "0.839 (+/-0.061) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 3}\n",
      "0.836 (+/-0.054) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 4}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       1.00      1.00      1.00         1\n",
      "\n",
      "avg / total       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.872 (+/-0.021) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 1}\n",
      "0.875 (+/-0.017) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 2}\n",
      "0.869 (+/-0.025) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 3}\n",
      "0.870 (+/-0.021) for {'loss': 'deviance', 'learning_rate': 0.10000000000000001, 'max_depth': 4}\n",
      "0.880 (+/-0.020) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 1}\n",
      "0.873 (+/-0.023) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 2}\n",
      "0.874 (+/-0.026) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 3}\n",
      "0.875 (+/-0.018) for {'loss': 'exponential', 'learning_rate': 0.10000000000000001, 'max_depth': 4}\n",
      "0.873 (+/-0.021) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 1}\n",
      "0.862 (+/-0.021) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 2}\n",
      "0.869 (+/-0.024) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 3}\n",
      "0.875 (+/-0.010) for {'loss': 'deviance', 'learning_rate': 0.20000000000000001, 'max_depth': 4}\n",
      "0.875 (+/-0.026) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 1}\n",
      "0.868 (+/-0.019) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 2}\n",
      "0.868 (+/-0.024) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 3}\n",
      "0.876 (+/-0.029) for {'loss': 'exponential', 'learning_rate': 0.20000000000000001, 'max_depth': 4}\n",
      "0.871 (+/-0.025) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 1}\n",
      "0.861 (+/-0.012) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 2}\n",
      "0.858 (+/-0.028) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 3}\n",
      "0.862 (+/-0.020) for {'loss': 'deviance', 'learning_rate': 0.30000000000000004, 'max_depth': 4}\n",
      "0.868 (+/-0.032) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 1}\n",
      "0.867 (+/-0.013) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 2}\n",
      "0.869 (+/-0.032) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 3}\n",
      "0.872 (+/-0.029) for {'loss': 'exponential', 'learning_rate': 0.30000000000000004, 'max_depth': 4}\n",
      "0.861 (+/-0.026) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 1}\n",
      "0.859 (+/-0.032) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 2}\n",
      "0.864 (+/-0.034) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 3}\n",
      "0.856 (+/-0.009) for {'loss': 'deviance', 'learning_rate': 0.40000000000000002, 'max_depth': 4}\n",
      "0.870 (+/-0.030) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 1}\n",
      "0.864 (+/-0.023) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 2}\n",
      "0.871 (+/-0.034) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 3}\n",
      "0.867 (+/-0.027) for {'loss': 'exponential', 'learning_rate': 0.40000000000000002, 'max_depth': 4}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       1.00      1.00      1.00         1\n",
      "\n",
      "avg / total       1.00      1.00      1.00         2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'loss': ['deviance','exponential'],'max_depth': [1,2,3,4], 'learning_rate' : np.arange(0.1,0.5,0.1) }]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    gbm = GradientBoostingClassifier(n_estimators=223)\n",
    "    clf = GridSearchCV(gbm, tuned_parameters, cv=5,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "original_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': None, 'random_state': 2,\n",
    "                   'min_samples_split': 5}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for label, color, setting in [('No shrinkage', 'orange',\n",
    "                               {'learning_rate': 1.0, 'subsample': 1.0}),\n",
    "                              ('learning_rate=0.1', 'turquoise',\n",
    "                               {'learning_rate': 0.1, 'subsample': 1.0}),\n",
    "                              ('subsample=0.5', 'blue',\n",
    "                               {'learning_rate': 1.0, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, subsample=0.5', 'gray',\n",
    "                               {'learning_rate': 0.1, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, max_features=2', 'magenta',\n",
    "                               {'learning_rate': 0.1, 'max_features': 2})]:\n",
    "    params = dict(original_params)\n",
    "    params.update(setting)\n",
    "\n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # compute test set deviance\n",
    "    test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        # clf.loss_ assumes that y_test[i] in {0, 1}\n",
    "        test_deviance[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "    plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],\n",
    "            '-', color=color, label=label)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Test Set Deviance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "\n",
    "data = pandas.read_excel('Vis.xlsx')\n",
    "a = Headings_sorted[:10]\n",
    "a.append('Lung_Cancer')\n",
    "data = data[a]\n",
    "parallel_coordinates(data,'Lung_Cancer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = Headings_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.append('Lung_Cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_coordinates(data, 'setosa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
